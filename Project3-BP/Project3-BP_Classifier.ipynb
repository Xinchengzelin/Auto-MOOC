{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# 自己代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "* 神经网络大小[3072, 200,80, 62] epochs=500, mini_batch_size=1, eta=0.45,运行了39.0mins，training_acc: 82.1%，train_cost约0.097,test_acc:约56%,test_cost:0.327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:62: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training, Acc: 0.00326726203441516, cost: 0.9799287255404499:\n",
      "Epoch 0 evaluate: 6 / 2534, Acc: 0.0023677979479084454, cost: 0.9764119156325322\n",
      "Epoch 1 training, Acc: 0.00326726203441516, cost: 0.9788993080409958:\n",
      "Epoch 1 evaluate: 6 / 2534, Acc: 0.0023677979479084454, cost: 0.9798358673589369\n",
      "Epoch 2 training, Acc: 0.00326726203441516, cost: 0.9570850026703241:\n",
      "Epoch 2 evaluate: 6 / 2534, Acc: 0.0023677979479084454, cost: 0.9468523744728647\n",
      "Epoch 3 training, Acc: 0.19255064256153342, cost: 0.4577245344125543:\n",
      "Epoch 3 evaluate: 680 / 2534, Acc: 0.26835043409629045, cost: 0.4308914431648413\n",
      "Epoch 4 training, Acc: 0.21520365933347854, cost: 0.4471292628700074:\n",
      "Epoch 4 evaluate: 672 / 2534, Acc: 0.26519337016574585, cost: 0.44268145710116513\n",
      "Epoch 5 training, Acc: 0.21759965149204966, cost: 0.4364255936070286:\n",
      "Epoch 5 evaluate: 667 / 2534, Acc: 0.2632202052091555, cost: 0.42343330339700286\n",
      "Epoch 6 training, Acc: 0.25049008930516226, cost: 0.4223572398068448:\n",
      "Epoch 6 evaluate: 852 / 2534, Acc: 0.3362273086029992, cost: 0.3954311162438367\n",
      "Epoch 7 training, Acc: 0.2607275103463298, cost: 0.4411109883328714:\n",
      "Epoch 7 evaluate: 858 / 2534, Acc: 0.33859510655090763, cost: 0.4104049322793205\n",
      "Epoch 8 training, Acc: 0.2321934219124374, cost: 0.43819313652796826:\n",
      "Epoch 8 evaluate: 678 / 2534, Acc: 0.2675611681136543, cost: 0.4322223748929875\n",
      "Epoch 9 training, Acc: 0.28207362230450883, cost: 0.41058344756605547:\n",
      "Epoch 9 evaluate: 887 / 2534, Acc: 0.3500394632991318, cost: 0.38718857439449117\n",
      "Epoch 10 training, Acc: 0.28425179699411895, cost: 0.417375373510922:\n",
      "Epoch 10 evaluate: 841 / 2534, Acc: 0.3318863456985004, cost: 0.39741335562098673\n",
      "Epoch 11 training, Acc: 0.28272707471139186, cost: 0.42911700960985466:\n",
      "Epoch 11 evaluate: 849 / 2534, Acc: 0.335043409629045, cost: 0.41112181942051634\n",
      "Epoch 12 training, Acc: 0.31322152036593337, cost: 0.39656624697675824:\n",
      "Epoch 12 evaluate: 832 / 2534, Acc: 0.3283346487766377, cost: 0.39594813702164017\n",
      "Epoch 13 training, Acc: 0.28033108255282074, cost: 0.4101376221117401:\n",
      "Epoch 13 evaluate: 591 / 2534, Acc: 0.23322809786898185, cost: 0.4392159200409317\n",
      "Epoch 14 training, Acc: 0.3387061642343716, cost: 0.4056920146572487:\n",
      "Epoch 14 evaluate: 897 / 2534, Acc: 0.35398579321231255, cost: 0.40188383451658316\n",
      "Epoch 15 training, Acc: 0.37028969723371813, cost: 0.3789100758283993:\n",
      "Epoch 15 evaluate: 935 / 2534, Acc: 0.3689818468823994, cost: 0.3820685941784166\n",
      "Epoch 16 training, Acc: 0.387279459812677, cost: 0.37020175298123054:\n",
      "Epoch 16 evaluate: 963 / 2534, Acc: 0.3800315706393054, cost: 0.3730459074143884\n",
      "Epoch 17 training, Acc: 0.37094314964060116, cost: 0.3937458741126585:\n",
      "Epoch 17 evaluate: 962 / 2534, Acc: 0.3796369376479874, cost: 0.3959989970896727\n",
      "Epoch 18 training, Acc: 0.37094314964060116, cost: 0.37537486066966824:\n",
      "Epoch 18 evaluate: 865 / 2534, Acc: 0.3413575374901342, cost: 0.38925281839793646\n",
      "Epoch 19 training, Acc: 0.38662600740579395, cost: 0.3600970701845359:\n",
      "Epoch 19 evaluate: 968 / 2534, Acc: 0.3820047355958958, cost: 0.3772965889246044\n",
      "Epoch 20 training, Acc: 0.4121106512742322, cost: 0.3551182529164974:\n",
      "Epoch 20 evaluate: 915 / 2534, Acc: 0.3610891870560379, cost: 0.38349914305394894\n",
      "Epoch 21 training, Acc: 0.40165541276410366, cost: 0.3581324846595623:\n",
      "Epoch 21 evaluate: 995 / 2534, Acc: 0.39265982636148383, cost: 0.368338756529771\n",
      "Epoch 22 training, Acc: 0.41733827052929645, cost: 0.3596766456086478:\n",
      "Epoch 22 evaluate: 961 / 2534, Acc: 0.3792423046566693, cost: 0.3885276796244116\n",
      "Epoch 23 training, Acc: 0.40514049226747983, cost: 0.35904343687550855:\n",
      "Epoch 23 evaluate: 965 / 2534, Acc: 0.3808208366219416, cost: 0.37415643587301955\n",
      "Epoch 24 training, Acc: 0.4053583097364409, cost: 0.35783109457689727:\n",
      "Epoch 24 evaluate: 897 / 2534, Acc: 0.35398579321231255, cost: 0.39038415697280554\n",
      "Epoch 25 training, Acc: 0.43999128730124154, cost: 0.3412875831937618:\n",
      "Epoch 25 evaluate: 1011 / 2534, Acc: 0.398973954222573, cost: 0.3708267083243279\n",
      "Epoch 26 training, Acc: 0.4243084295360488, cost: 0.3559712938946876:\n",
      "Epoch 26 evaluate: 993 / 2534, Acc: 0.39187056037884765, cost: 0.38787066033176487\n",
      "Epoch 27 training, Acc: 0.4208233500326726, cost: 0.3597114447883139:\n",
      "Epoch 27 evaluate: 1037 / 2534, Acc: 0.40923441199684296, cost: 0.38394560465879335\n",
      "Epoch 28 training, Acc: 0.4426050969287737, cost: 0.3323034288628807:\n",
      "Epoch 28 evaluate: 1005 / 2534, Acc: 0.39660615627466456, cost: 0.3742080762409046\n",
      "Epoch 29 training, Acc: 0.44717926377695494, cost: 0.33168734807223105:\n",
      "Epoch 29 evaluate: 1081 / 2534, Acc: 0.4265982636148382, cost: 0.35987337649452955\n",
      "Epoch 30 training, Acc: 0.46046612938357656, cost: 0.3288845966837627:\n",
      "Epoch 30 evaluate: 1045 / 2534, Acc: 0.4123914759273875, cost: 0.3686168046506684\n",
      "Epoch 31 training, Acc: 0.4452189065563058, cost: 0.33233062242848177:\n",
      "Epoch 31 evaluate: 972 / 2534, Acc: 0.3835832675611681, cost: 0.37239967602631224\n",
      "Epoch 32 training, Acc: 0.44216946199085166, cost: 0.3310910459666461:\n",
      "Epoch 32 evaluate: 1022 / 2534, Acc: 0.40331491712707185, cost: 0.374616414200225\n",
      "Epoch 33 training, Acc: 0.46024831191461557, cost: 0.32539973455873716:\n",
      "Epoch 33 evaluate: 1037 / 2534, Acc: 0.40923441199684296, cost: 0.36711650544712565\n",
      "Epoch 34 training, Acc: 0.42125898497059466, cost: 0.39567564510699094:\n",
      "Epoch 34 evaluate: 930 / 2534, Acc: 0.367008681925809, cost: 0.4315781859675574\n",
      "Epoch 35 training, Acc: 0.46656501851448484, cost: 0.32837968522450006:\n",
      "Epoch 35 evaluate: 1078 / 2534, Acc: 0.425414364640884, cost: 0.36492691508286984\n",
      "Epoch 36 training, Acc: 0.4369418427357874, cost: 0.3368122658166675:\n",
      "Epoch 36 evaluate: 1028 / 2534, Acc: 0.40568271507498027, cost: 0.3725338955450737\n",
      "Epoch 37 training, Acc: 0.4711391853626661, cost: 0.31951806063633725:\n",
      "Epoch 37 evaluate: 1060 / 2534, Acc: 0.4183109707971586, cost: 0.36411400937770766\n",
      "Epoch 38 training, Acc: 0.469396645610978, cost: 0.3276843963640656:\n",
      "Epoch 38 evaluate: 1078 / 2534, Acc: 0.425414364640884, cost: 0.3736185568231003\n",
      "Epoch 39 training, Acc: 0.4552385101285123, cost: 0.330329419246106:\n",
      "Epoch 39 evaluate: 1049 / 2534, Acc: 0.4139700078926598, cost: 0.3646701801390916\n",
      "Epoch 40 training, Acc: 0.4750598998039643, cost: 0.3111095131185888:\n",
      "Epoch 40 evaluate: 1094 / 2534, Acc: 0.43172849250197315, cost: 0.3561295823442054\n",
      "Epoch 41 training, Acc: 0.46395120888695274, cost: 0.3389636294706741:\n",
      "Epoch 41 evaluate: 1048 / 2534, Acc: 0.4135753749013418, cost: 0.39050329963603936\n",
      "Epoch 42 training, Acc: 0.46199085166630366, cost: 0.3435810118390246:\n",
      "Epoch 42 evaluate: 1118 / 2534, Acc: 0.44119968429360695, cost: 0.3801216742101072\n",
      "Epoch 43 training, Acc: 0.4726639076453932, cost: 0.31710753433439043:\n",
      "Epoch 43 evaluate: 1122 / 2534, Acc: 0.44277821625887925, cost: 0.3559281016489878\n",
      "Epoch 44 training, Acc: 0.4615552167283816, cost: 0.34041187661421374:\n",
      "Epoch 44 evaluate: 1032 / 2534, Acc: 0.40726124704025257, cost: 0.3934712615989087\n",
      "Epoch 45 training, Acc: 0.4998910912655195, cost: 0.31170113730723215:\n",
      "Epoch 45 evaluate: 1110 / 2534, Acc: 0.43804262036306235, cost: 0.3535012344501343\n",
      "Epoch 46 training, Acc: 0.48224787627967763, cost: 0.32427566108375755:\n",
      "Epoch 46 evaluate: 1082 / 2534, Acc: 0.42699289660615625, cost: 0.3784707329893003\n",
      "Epoch 47 training, Acc: 0.47309954258331516, cost: 0.3470935117155124:\n",
      "Epoch 47 evaluate: 1031 / 2534, Acc: 0.4068666140489345, cost: 0.4047706774709963\n",
      "Epoch 48 training, Acc: 0.5029405358309736, cost: 0.307614467733191:\n",
      "Epoch 48 evaluate: 1074 / 2534, Acc: 0.4238358326756117, cost: 0.3661834469930667\n",
      "Epoch 49 training, Acc: 0.5345240688303202, cost: 0.28495420093052604:\n",
      "Epoch 49 evaluate: 1150 / 2534, Acc: 0.4538279400157853, cost: 0.358949216866405\n",
      "process time: 4.74954205205892 mins\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from get_data import read_data, to_grayscale, normalise_images\n",
    "import matplotlib.pyplot as plt\n",
    "import time#计算代码运行时间\n",
    "\n",
    "\n",
    "class dense(object):\n",
    "    def __init__(self, x, neurons, activation):\n",
    "        \"\"\"\n",
    "        全连接层的实现\n",
    "        :param x: 当前层的输入\n",
    "        :param neurons: 神经元数目\n",
    "        :param activation: 神经元激活函数\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.activation = activation\n",
    "        # 初始化bias，shape=(neurons, 1)\n",
    "        self.biases = np.random.randn(neurons, 1)\n",
    "        # 初始化权重，shape=(x, neurons)\n",
    "        self.weights = np.random.randn(len(self.x), neurons)\n",
    "        self.a = np.zeros(neurons)#\n",
    "        self.z = np.zeros(neurons)#\n",
    "\n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播实现\n",
    "        z=wx+b    a=activation(z)\n",
    "        :param x: 当前层的输入\n",
    "        :return: 当前层的激活值\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.z = np.dot(self.weights.transpose(), self.x) + self.biases\n",
    "        self.a = self.activation.activate(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def backprop(self):\n",
    "        \"\"\"\n",
    "        计算梯度\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        grad = self.activation.prime(self.z)\n",
    "        return grad\n",
    "\n",
    "    def back(self, delta_w, delta_b):\n",
    "        \"\"\"\n",
    "        更新weights和bias\n",
    "        :param delta_w: 权重变化大小矩阵\n",
    "        :param delta_b: 偏置变化大小矩阵\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.weights = self.weights - delta_w\n",
    "        self.biases = self.biases - delta_b\n",
    "\n",
    "\n",
    "class sigmoid(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def activate(z):\n",
    "        \"\"\"The sigmoid function.\"\"\"\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def prime(z):\n",
    "        \"\"\"sigmoid函数的微分.\"\"\"\n",
    "        return sigmoid.activate(z) * (1 - sigmoid.activate(z))\n",
    "\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0.5 * np.linalg.norm(a - y) ** 2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a - y)\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    def __init__(self, sizes, cost=QuadraticCost):\n",
    "        \"\"\"\n",
    "        神经网络结构定义\n",
    "        :param sizes: 神经网络的大小；（784, 30, 10）即表示输入层是784个节点；隐藏层是30个节点；输出是10个节点\n",
    "        :param cost: 代价函数\n",
    "        \"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.cost = cost\n",
    "        self.sizes = sizes\n",
    "        self.x = np.zeros(self.sizes[0])#输入层节点数\n",
    "        self.fc_layers = []\n",
    "        x = self.x\n",
    "        #         print('self.x:',x.shape,x)\n",
    "        # 创建隐藏层和输出层\n",
    "        for neurons in self.sizes[1:]:\n",
    "            self.fc_layers.append(dense(x, neurons, sigmoid))\n",
    "            x = self.fc_layers[-1].a\n",
    "#             print('x:',x)\n",
    "#             print('self.fc_layers:',len(self.fc_layers),self.fc_layers[len(self.fc_layers)-1])\n",
    "\n",
    "\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "        前向传播，计算网络的输出\n",
    "        :param a: 网络输入\n",
    "        :return: 网络输出\n",
    "        \"\"\"\n",
    "        x = a\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer.feedforward(x)\n",
    "        return self.fc_layers[-1].a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            evaluation_data=None):\n",
    "        \"\"\"\n",
    "        随机梯度下降\n",
    "        :param training_data: 训练集\n",
    "        :param epochs: 训练周期\n",
    "        :param mini_batch_size: 批次大小\n",
    "        :param eta: 学习率参数 ，真实学习率lr=eta/len(mini_batch_size)\n",
    "        :param evaluation_data: 验证集\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        train_accs = []\n",
    "        train_costs = []\n",
    "\n",
    "        test_accs = []\n",
    "        test_costs = []\n",
    "        if evaluation_data: n_data = len(evaluation_data)\n",
    "        n = 4591#len(training_data)\n",
    "        for j in range(epochs):\n",
    "            # 数据随机化\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k:k + mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "#             print('mini_batches:',len(mini_batches),mini_batches[0])\n",
    "            # 针对每一个批次进行梯度下降\n",
    "            for mini_batch in mini_batches:\n",
    "#                 print('mini_batch:',np.array(mini_batch).shape,mini_batch)\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "                \n",
    "\n",
    "            train_acc = self.evaluate(training_data, True) / n\n",
    "            train_accs.append(train_acc)\n",
    "            train_cost = self.total_cost(training_data)\n",
    "            train_costs.append(train_cost)\n",
    "            print(\"Epoch {} training, Acc: {}, cost: {}:\".format(j, train_acc, train_cost))\n",
    "            # 打印出正确分类的个数\n",
    "            if evaluation_data:\n",
    "                test_correct_count = self.evaluate(evaluation_data)\n",
    "                test_acc = test_correct_count/n_data\n",
    "                test_cost = self.total_cost(evaluation_data)#去掉, convert=True\n",
    "                test_accs.append(test_acc)\n",
    "                test_costs.append(test_cost)\n",
    "                print(\"Epoch {} evaluate: {} / {}, Acc: {}, cost: {}\".format(j, self.evaluate(evaluation_data), n_data, test_acc, test_cost))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "        return train_accs, train_costs, test_accs, test_costs\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"\n",
    "        梯度下降，向后传播更新参数\n",
    "        :param mini_batch: 批次数据\n",
    "        :param eta: 学习率参数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        lr = eta / len(mini_batch)\n",
    "        for x, y in mini_batch:\n",
    "            # 前向传播计算每层的输出\n",
    "            self.feedforward(x)\n",
    "            # 计算输出层的梯度\n",
    "            delta = self.cost.delta(self.fc_layers[-1].a, y) * self.fc_layers[-1].backprop()\n",
    "            delta_w = np.dot(self.fc_layers[-2].a, delta.transpose())\n",
    "            self.fc_layers[-1].back(lr * delta_w, eta / lr * delta)\n",
    "            # 当网络结构多一个隐藏层\n",
    "            if self.num_layers > 3:\n",
    "                for layer in range(2, self.num_layers - 1):\n",
    "                    delta = np.dot(self.fc_layers[-layer + 1].weights, delta) * self.fc_layers[\n",
    "                        -layer].backprop()\n",
    "                    delta_w = np.dot(self.fc_layers[-layer - 1].a, delta.transpose())\n",
    "                    self.fc_layers[-layer].back(lr * delta_w, lr * delta)\n",
    "\n",
    "            # 三层结构\n",
    "            else:\n",
    "                delta = np.dot(self.fc_layers[-1].weights, delta) * self.fc_layers[\n",
    "                    -2].backprop()\n",
    "                delta_w = np.dot(x, delta.transpose())\n",
    "                self.fc_layers[-2].back(lr * delta_w, lr * delta)\n",
    "\n",
    "    def evaluate(self, test_data, convert=False):\n",
    "        \"\"\"统计出来正确分类的个数;argmax表示数组中最大的值的位置；因为最终output的结果是0,1向量，只有一个值为1，也就是判别的种类\"\"\"\n",
    "        if not convert:\n",
    "            test_results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                            for (x, y) in test_data]\n",
    "#             print('False:',len(test_results),len(test_results[0]),test_results)\n",
    "        else:\n",
    "            test_results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in test_data]\n",
    "#             print('True:',len(test_results),len(test_results[0]),test_results)\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def total_cost(self, data, convert=False):\n",
    "        \"\"\"\n",
    "        所有的损失值\n",
    "        \"\"\"\n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y) / len(data)\n",
    "        return cost\n",
    "\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 62-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((62, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    #读取数据，并将数据转换为可输入BP神经网络的格式\n",
    "    train_x, train_y = read_data('./data/BelgiumTSC_Training.zip', 'Training/')\n",
    "#     print('training data shape:',train_x.shape,train_y.shape)#train_x大小为(4591, 32, 32, 3),train_y大小为4591*1\n",
    "    #将训练集转换为可输入BP神经网络的格式\n",
    "    training_inputs = [np.reshape(x, (3072, 1)) for x in train_x]\n",
    "#     print(len(training_inputs),len(training_inputs[0]))\n",
    "    training_results = [vectorized_result(y) for y in train_y]\n",
    "#     print(len(training_results),len(training_results[0]))\n",
    "    training_data= list(zip (training_inputs, training_results))\n",
    "#     print(len(training_data),len(training_data[0]),training_data[0])\n",
    "    #将测试集转换为可输入BP神经网络的格式\n",
    "    test_x, test_y = read_data('./data/BelgiumTSC_Testing.zip', 'Testing/')\n",
    "#     print('test data shape:',test_x.shape,test_y.shape)#test_x大小为(2534, 32, 32, 3),test_y大小为2534*1\n",
    "    test_inputs = [np.reshape(x, (3072, 1)) for x in test_x]\n",
    "#     print(len(test_inputs),len(test_inputs[0]))\n",
    "    test_results = [vectorized_result(y) for y in test_y]\n",
    "#     print(len(test_results),len(test_results[0]))\n",
    "    test_data= list(zip (test_inputs, test_results))\n",
    "#     print(len(test_data),len(test_data[0]),test_data[0])\n",
    "    \n",
    "    #神经网络模型\n",
    "    Model=Network([3072, 200,80, 62], cost=QuadraticCost)#Model为神经网路模型，输入层是3072个节点；输出层是62个节点；隐藏层初始值≈sqrt(3072*62）个节点；\n",
    "    train_accs, train_costs, test_accs, test_costs=Model.SGD(training_data=training_data, epochs=50, mini_batch_size=1, eta=0.45,evaluation_data=test_data)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start = time.clock()\n",
    "    main()\n",
    "    end = time.clock()\n",
    "    print('process time:',str((end - start)/60),'mins')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试结果\n",
    "\n",
    "### 神经网络大小[3072, 436, 62]\n",
    "* epochs=50, mini_batch_size=1, eta=0.1 计算很慢，training_data的准确率只有约2.1%\n",
    "* epochs=200, mini_batch_size=10, eta=0.5,运行了6个小时，training_data的准确率最高只有约4.8%\n",
    "* epochs=10, mini_batch_size=10, eta=1,运行了18mins，training_data的准确率最高只有约0.9%\n",
    "* epochs=10, mini_batch_size=1, eta=1,运行了18mins，training_data的准确率最高只有约4.2%\n",
    "\n",
    "### 神经网络大小[3072, 200, 62]\n",
    "* epochs=10, mini_batch_size=1, eta=1,运行了7.8mins，training_data的准确率最高只有约1.8%\n",
    "\n",
    "### 神经网络大小[3072, 600, 62]\n",
    "* epochs=10, mini_batch_size=1, eta=1,运行了  mins，training_data的准确率最高只有约6.9%\n",
    "\n",
    "### 神经网络大小[3072, 1024, 62]\n",
    "* epochs=2, mini_batch_size=1, eta=1，运行了  mins，training_data的准确率最高约8.1%\n",
    "\n",
    "### 神经网络大小[3072, 200,100, 62]\n",
    "* epochs=20, mini_batch_size=1, eta=1，运行了1.2mins，training_data的准确率最高约5.6%\n",
    "* epochs=20, mini_batch_size=1, eta=0.5,运行了1.2mins，training_data的准确率最高约47.0%\n",
    "* epochs=20, mini_batch_size=1, eta=0.6,运行了1.2mins，training_data的准确率最高约42.1%\n",
    "* epochs=50, mini_batch_size=1, eta=0.5,运行了3.2mins，training_data的准确率最高约53.4%\n",
    "\n",
    "### 神经网络大小[3072, 200,80, 62]\n",
    "* epochs=10, mini_batch_size=1, eta=0.5,运行了0.71mins，training_data的准确率最高约44.5%\n",
    "* epochs=50, mini_batch_size=1, eta=0.5,运行了2.6mins，training_data的准确率最高约49.9%,train_cost约0.33, test_acc:50%,test_cost:0.35\n",
    "* epochs=50, mini_batch_size=1, eta=0.45,运行了4.7mins，training_data的准确率最高约53.4%,train_cost约0.28, test_acc:45.3%,test_cost:0.36 \n",
    "* epochs=200, mini_batch_size=1, eta=0.5,运行了10.5mins，training_data的准确率最高约70.6%\n",
    "* epochs=500, mini_batch_size=1, eta=0.45,运行了39.0mins，training_acc: 82.1%，train_cost约0.097,test_acc:约56%,test_cost:0.327\n",
    "\n",
    "### 神经网络大小[3072, 200,70, 62]\n",
    "* epochs=50, mini_batch_size=1, eta=0.5,运行了2.7mins，training_data的准确率最高约63.0%，train_cost约0.26\n",
    "* epochs=500, mini_batch_size=1, eta=0.45，运行了25.7mins，training_data的准确率最高约77.9%，train_cost约0.11 \n",
    "\n",
    "### 神经网络大小[3072, 200,62, 62]\n",
    "* epochs=50, mini_batch_size=1, eta=0.45,运行了3.7mins，training_acc:66.1%，train_cost约0.21,test_acc:约53.2%,test_cost:0.31 \n",
    "\n",
    "### 神经网络大小[3072, 200,50, 62]\n",
    "* epochs=50, mini_batch_size=1, eta=0.45,运行了3.4mins，training_acc:65.2%，train_cost约0.26,test_acc:约53.7%,test_cost:0.33\n",
    "\n",
    "### 神经网络大小[3072, 200,30, 62]\n",
    "* epochs=50, mini_batch_size=1, eta=0.45,运行了3.2mins，training_acc:61.2%，train_cost约0.26,test_acc:约49.6%,test_cost: 0.33\n",
    "\n",
    "### 神经网络大小[3072, 250,80, 62]\n",
    "* epochs=50, mini_batch_size=1, eta=0.5,运行了4.2mins，training_data的准确率最高约0.6%，train_cost约0.76\n",
    "\n",
    "### 神经网络大小[3072, 180,80, 62]\n",
    "* epochs=50, mini_batch_size=1, eta=0.5,运行了2.3mins，training_data的准确率最高约60.0%，train_cost约0.25\n",
    "\n",
    "### 神经网络大小[3072, 150,80, 62]\n",
    "* epochs=50, mini_batch_size=1, eta=0.5,运行了2.0mins，training_data的准确率最高约63.4%，train_cost约0.24\n",
    "* epochs=500, mini_batch_size=1, eta=0.45，运行了27.6mins，training_acc: 76.6%,train_cost约0.118, test_acc:52.9%,test_cost:0.34\n",
    "\n",
    "### 神经网络大小[3072, 120,80, 62]\n",
    "* epochs=50, mini_batch_size=1, eta=0.5,运行了1.6mins，training_data的准确率最高约60.2%，train_cost约0.25 \n",
    "\n",
    "### 神经网络大小[3072, 100,80, 62]\n",
    "* epochs=50,mini_batch_size=1,eta=0.5,运行了1.4mins，training_data的准确率最高约51.2%，train_cost约0.31\n",
    "* epochs=500,mini_batch_size=1,eta=0.45,运行了23.2mins，training_acc:73.1%，train_cost约0.135，test_acc:约52%, test_cost: 0.363\n",
    "\n",
    "### 神经网络大小[3072,400,100,62]\n",
    "* epochs=10, mini_batch_size=1, eta=0.7,运行了2.6mins，training_data的准确率最高约26.9%\n",
    "\n",
    "### 神经网络大小[3072, 400, 200, 62]\n",
    "* epochs=5, mini_batch_size=1, eta=0.5,运行了1.54mins，training_data的准确率最高约0.2%\n",
    "* epochs=5, mini_batch_size=1, eta=1,运行了1.54mins，training_data的准确率最高约5.6%\n",
    "\n",
    "### 神经网络大小[3072, 300, 200,100, 62]\n",
    "* epochs=10, mini_batch_size=1, eta=0.3,运行了1.8mins，training_data的准确率最高约39%\n",
    "\n",
    "### 神经网络大小[3072, 400, 200,100,62]\n",
    "* epochs=10, mini_batch_size=1, eta=0.5,运行了3.3mins，training_data的准确率最高约28.4%\n",
    "* epochs=50, mini_batch_size=1, eta=0.5,运行了16.4mins，training_data的准确率最高约32.7%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 上面的代码两层神经网络（只有一个隐藏层），计算时间极长；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 使用TensorFlow实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论\n",
    "* 网络大小[3072,600,62],输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了28.6mins，train_acc:96.3% ,test_acc:70.4%\n",
    "* 层数越多，中间层越大，越容易过拟合；但是Training_acc越高，一般情况Test_acc也会略高一点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_acc:  0 0.026791548\n",
      "Test_acc:  0 0.044988163\n",
      "Train_acc:  10 0.13613592\n",
      "Test_acc:  10 0.10773481\n",
      "Train_acc:  20 0.21607493\n",
      "Test_acc:  20 0.17363852\n",
      "Train_acc:  30 0.28839034\n",
      "Test_acc:  30 0.22296764\n",
      "Train_acc:  40 0.34001306\n",
      "Test_acc:  40 0.2573007\n",
      "Train_acc:  50 0.38989326\n",
      "Test_acc:  50 0.28295186\n",
      "Train_acc:  60 0.42038772\n",
      "Test_acc:  60 0.3180742\n",
      "Train_acc:  70 0.45698106\n",
      "Test_acc:  70 0.34411997\n",
      "Train_acc:  80 0.47636682\n",
      "Test_acc:  80 0.3670087\n",
      "Train_acc:  90 0.50315833\n",
      "Test_acc:  90 0.37529597\n",
      "Train_acc:  100 0.5223263\n",
      "Test_acc:  100 0.39029202\n",
      "Train_acc:  110 0.53953385\n",
      "Test_acc:  110 0.40213102\n",
      "Train_acc:  120 0.5597909\n",
      "Test_acc:  120 0.41397\n",
      "Train_acc:  130 0.5759094\n",
      "Test_acc:  130 0.43133387\n",
      "Train_acc:  140 0.59812677\n",
      "Test_acc:  140 0.43725336\n",
      "Train_acc:  150 0.60509694\n",
      "Test_acc:  150 0.44672453\n",
      "Train_acc:  160 0.62404704\n",
      "Test_acc:  160 0.4558011\n",
      "Train_acc:  170 0.6355914\n",
      "Test_acc:  170 0.45777428\n",
      "Train_acc:  180 0.643215\n",
      "Test_acc:  180 0.46724546\n",
      "Train_acc:  190 0.6595513\n",
      "Test_acc:  190 0.46921864\n",
      "Train_acc:  200 0.66935307\n",
      "Test_acc:  200 0.47790056\n",
      "Train_acc:  210 0.68176866\n",
      "Test_acc:  210 0.4822415\n",
      "Train_acc:  220 0.690917\n",
      "Test_acc:  220 0.48618785\n",
      "Train_acc:  230 0.6946199\n",
      "Test_acc:  230 0.4842147\n",
      "Train_acc:  240 0.70834243\n",
      "Test_acc:  240 0.49171272\n",
      "Train_acc:  250 0.7190155\n",
      "Test_acc:  250 0.49684295\n",
      "Train_acc:  260 0.72293615\n",
      "Test_acc:  260 0.49842146\n",
      "Train_acc:  270 0.72555\n",
      "Test_acc:  270 0.5031571\n",
      "Train_acc:  280 0.7331736\n",
      "Test_acc:  280 0.51578534\n",
      "Train_acc:  290 0.7412329\n",
      "Test_acc:  290 0.5260458\n",
      "Train_acc:  300 0.7477674\n",
      "Test_acc:  300 0.51736385\n",
      "Train_acc:  310 0.757787\n",
      "Test_acc:  310 0.5185478\n",
      "Train_acc:  320 0.76519275\n",
      "Test_acc:  320 0.5264404\n",
      "Train_acc:  330 0.7680244\n",
      "Test_acc:  330 0.5299921\n",
      "Train_acc:  340 0.7741233\n",
      "Test_acc:  340 0.53433305\n",
      "Train_acc:  350 0.78588545\n",
      "Test_acc:  350 0.5370955\n",
      "Train_acc:  360 0.7841429\n",
      "Test_acc:  360 0.5406472\n",
      "Train_acc:  370 0.79285556\n",
      "Test_acc:  370 0.54656667\n",
      "Train_acc:  380 0.7978654\n",
      "Test_acc:  380 0.54814523\n",
      "Train_acc:  390 0.8041821\n",
      "Test_acc:  390 0.550513\n",
      "Train_acc:  400 0.8098453\n",
      "Test_acc:  400 0.55880034\n",
      "Train_acc:  410 0.81398386\n",
      "Test_acc:  410 0.55643255\n",
      "Train_acc:  420 0.81594425\n",
      "Test_acc:  420 0.5568271\n",
      "Train_acc:  430 0.82269657\n",
      "Test_acc:  430 0.56274664\n",
      "Train_acc:  440 0.8224788\n",
      "Test_acc:  440 0.5690608\n",
      "Train_acc:  450 0.8298845\n",
      "Test_acc:  450 0.5670876\n",
      "Train_acc:  460 0.83511215\n",
      "Test_acc:  460 0.5737964\n",
      "Train_acc:  470 0.837726\n",
      "Test_acc:  470 0.57576954\n",
      "Train_acc:  480 0.8399042\n",
      "Test_acc:  480 0.578532\n",
      "Train_acc:  490 0.8442605\n",
      "Test_acc:  490 0.578532\n",
      "Train_acc:  500 0.8479634\n",
      "Test_acc:  500 0.5852407\n",
      "Train_acc:  510 0.84600306\n",
      "Test_acc:  510 0.58445144\n",
      "Train_acc:  520 0.8501416\n",
      "Test_acc:  520 0.5828729\n",
      "Train_acc:  530 0.8564583\n",
      "Test_acc:  530 0.5824783\n",
      "Train_acc:  540 0.85950774\n",
      "Test_acc:  540 0.58681923\n",
      "Train_acc:  550 0.8629928\n",
      "Test_acc:  550 0.58800316\n",
      "Train_acc:  560 0.8642997\n",
      "Test_acc:  560 0.58879244\n",
      "Train_acc:  570 0.8677848\n",
      "Test_acc:  570 0.59431726\n",
      "Train_acc:  580 0.869963\n",
      "Test_acc:  580 0.5907656\n",
      "Train_acc:  590 0.8730124\n",
      "Test_acc:  590 0.59273875\n",
      "Train_acc:  600 0.8732302\n",
      "Test_acc:  600 0.5923441\n",
      "Train_acc:  610 0.8751906\n",
      "Test_acc:  610 0.59629047\n",
      "Train_acc:  620 0.87627965\n",
      "Test_acc:  620 0.597869\n",
      "Train_acc:  630 0.8788935\n",
      "Test_acc:  630 0.5966851\n",
      "Train_acc:  640 0.8815073\n",
      "Test_acc:  640 0.5970797\n",
      "Train_acc:  650 0.88172513\n",
      "Test_acc:  650 0.6018153\n",
      "Train_acc:  660 0.88368547\n",
      "Test_acc:  660 0.59984213\n",
      "Train_acc:  670 0.88586366\n",
      "Test_acc:  670 0.5994475\n",
      "Train_acc:  680 0.8886953\n",
      "Test_acc:  680 0.6018153\n",
      "Train_acc:  690 0.8900022\n",
      "Test_acc:  690 0.6069455\n",
      "Train_acc:  700 0.89022\n",
      "Test_acc:  700 0.6069455\n",
      "Train_acc:  710 0.8934873\n",
      "Test_acc:  710 0.60734016\n",
      "Train_acc:  720 0.8954476\n",
      "Test_acc:  720 0.60655093\n",
      "Train_acc:  730 0.89697236\n",
      "Test_acc:  730 0.6085241\n",
      "Train_acc:  740 0.89849705\n",
      "Test_acc:  740 0.60970795\n",
      "Train_acc:  750 0.9000218\n",
      "Test_acc:  750 0.61049724\n",
      "Train_acc:  760 0.9015465\n",
      "Test_acc:  760 0.6089187\n",
      "Train_acc:  770 0.90176433\n",
      "Test_acc:  770 0.6136543\n",
      "Train_acc:  780 0.9041603\n",
      "Test_acc:  780 0.6136543\n",
      "Train_acc:  790 0.9052494\n",
      "Test_acc:  790 0.6152328\n",
      "Train_acc:  800 0.9065563\n",
      "Test_acc:  800 0.6160221\n",
      "Train_acc:  810 0.90829885\n",
      "Test_acc:  810 0.617206\n",
      "Train_acc:  820 0.9091701\n",
      "Test_acc:  820 0.6160221\n",
      "Train_acc:  830 0.9111305\n",
      "Test_acc:  830 0.6168114\n",
      "Train_acc:  840 0.9126552\n",
      "Test_acc:  840 0.6199684\n",
      "Train_acc:  850 0.9143977\n",
      "Test_acc:  850 0.6191792\n",
      "Train_acc:  860 0.91614026\n",
      "Test_acc:  860 0.6183899\n",
      "Train_acc:  870 0.91810066\n",
      "Test_acc:  870 0.6191792\n",
      "Train_acc:  880 0.91940755\n",
      "Test_acc:  880 0.6199684\n",
      "Train_acc:  890 0.9191897\n",
      "Test_acc:  890 0.62036306\n",
      "Train_acc:  900 0.9198432\n",
      "Test_acc:  900 0.62352014\n",
      "Train_acc:  910 0.92071444\n",
      "Test_acc:  910 0.6207577\n",
      "Train_acc:  920 0.922457\n",
      "Test_acc:  920 0.6195738\n",
      "Train_acc:  930 0.9226748\n",
      "Test_acc:  930 0.62352014\n",
      "Train_acc:  940 0.92485297\n",
      "Test_acc:  940 0.6239148\n",
      "Train_acc:  950 0.92594206\n",
      "Test_acc:  950 0.62588793\n",
      "Train_acc:  960 0.9274668\n",
      "Test_acc:  960 0.6254933\n",
      "Train_acc:  970 0.92724895\n",
      "Test_acc:  970 0.6262826\n",
      "Train_acc:  980 0.9287737\n",
      "Test_acc:  980 0.6266772\n",
      "Train_acc:  990 0.92920935\n",
      "Test_acc:  990 0.62746644\n",
      "Train_acc:  1000 0.9298628\n",
      "Test_acc:  1000 0.629045\n",
      "Train_acc:  1010 0.9311697\n",
      "Test_acc:  1010 0.629045\n",
      "Train_acc:  1020 0.9324766\n",
      "Test_acc:  1020 0.63101816\n",
      "Train_acc:  1030 0.9324766\n",
      "Test_acc:  1030 0.6302289\n",
      "Train_acc:  1040 0.9337835\n",
      "Test_acc:  1040 0.6314128\n",
      "Train_acc:  1050 0.9344369\n",
      "Test_acc:  1050 0.63101816\n",
      "Train_acc:  1060 0.93574387\n",
      "Test_acc:  1060 0.63101816\n",
      "Train_acc:  1070 0.93617946\n",
      "Test_acc:  1070 0.632202\n",
      "Train_acc:  1080 0.93705076\n",
      "Test_acc:  1080 0.63180745\n",
      "Train_acc:  1090 0.9377042\n",
      "Test_acc:  1090 0.63180745\n",
      "Train_acc:  1100 0.93835765\n",
      "Test_acc:  1100 0.63417524\n",
      "Train_acc:  1110 0.9387933\n",
      "Test_acc:  1110 0.63496447\n",
      "Train_acc:  1120 0.93857545\n",
      "Test_acc:  1120 0.63417524\n",
      "Train_acc:  1130 0.93922895\n",
      "Test_acc:  1130 0.63575375\n",
      "Train_acc:  1140 0.9411893\n",
      "Test_acc:  1140 0.63575375\n",
      "Train_acc:  1150 0.94162494\n",
      "Test_acc:  1150 0.63812155\n",
      "Train_acc:  1160 0.94206053\n",
      "Test_acc:  1160 0.63733226\n",
      "Train_acc:  1170 0.9424962\n",
      "Test_acc:  1170 0.6393055\n",
      "Train_acc:  1180 0.94293183\n",
      "Test_acc:  1180 0.6393055\n",
      "Train_acc:  1190 0.9431496\n",
      "Test_acc:  1190 0.63891083\n",
      "Train_acc:  1200 0.9438031\n",
      "Test_acc:  1200 0.63891083\n",
      "Train_acc:  1210 0.9446744\n",
      "Test_acc:  1210 0.6393055\n",
      "Train_acc:  1220 0.94489217\n",
      "Test_acc:  1220 0.6393055\n",
      "Train_acc:  1230 0.9464169\n",
      "Test_acc:  1230 0.63970006\n",
      "Train_acc:  1240 0.9464169\n",
      "Test_acc:  1240 0.63970006\n",
      "Train_acc:  1250 0.9477238\n",
      "Test_acc:  1250 0.64048934\n",
      "Train_acc:  1260 0.94837725\n",
      "Test_acc:  1260 0.6424625\n",
      "Train_acc:  1270 0.94968414\n",
      "Test_acc:  1270 0.64206785\n",
      "Train_acc:  1280 0.95055544\n",
      "Test_acc:  1280 0.64285713\n",
      "Train_acc:  1290 0.95055544\n",
      "Test_acc:  1290 0.64285713\n",
      "Train_acc:  1300 0.95055544\n",
      "Test_acc:  1300 0.64404106\n",
      "Train_acc:  1310 0.95164454\n",
      "Test_acc:  1310 0.6452249\n",
      "Train_acc:  1320 0.952298\n",
      "Test_acc:  1320 0.6456196\n",
      "Train_acc:  1330 0.9531692\n",
      "Test_acc:  1330 0.64640886\n",
      "Train_acc:  1340 0.9533871\n",
      "Test_acc:  1340 0.6468035\n",
      "Train_acc:  1350 0.9538227\n",
      "Test_acc:  1350 0.6468035\n",
      "Train_acc:  1360 0.9542583\n",
      "Test_acc:  1360 0.6471981\n",
      "Train_acc:  1370 0.954694\n",
      "Test_acc:  1370 0.6468035\n",
      "Train_acc:  1380 0.9553474\n",
      "Test_acc:  1380 0.6475927\n",
      "Train_acc:  1390 0.95600086\n",
      "Test_acc:  1390 0.648382\n",
      "Train_acc:  1400 0.9566543\n",
      "Test_acc:  1400 0.6491713\n",
      "Train_acc:  1410 0.9566543\n",
      "Test_acc:  1410 0.6491713\n",
      "Train_acc:  1420 0.9575256\n",
      "Test_acc:  1420 0.6495659\n",
      "Train_acc:  1430 0.9579612\n",
      "Test_acc:  1430 0.65035516\n",
      "Train_acc:  1440 0.95839685\n",
      "Test_acc:  1440 0.65114444\n",
      "Train_acc:  1450 0.95839685\n",
      "Test_acc:  1450 0.65114444\n",
      "Train_acc:  1460 0.9590503\n",
      "Test_acc:  1460 0.6515391\n",
      "Train_acc:  1470 0.95926815\n",
      "Test_acc:  1470 0.6523283\n",
      "Train_acc:  1480 0.95948595\n",
      "Test_acc:  1480 0.65272295\n",
      "Train_acc:  1490 0.95970374\n",
      "Test_acc:  1490 0.6531176\n",
      "Train_acc:  1500 0.96057504\n",
      "Test_acc:  1500 0.65351224\n",
      "Train_acc:  1510 0.9599216\n",
      "Test_acc:  1510 0.65509075\n",
      "Train_acc:  1520 0.96057504\n",
      "Test_acc:  1520 0.6554854\n",
      "Train_acc:  1530 0.96188194\n",
      "Test_acc:  1530 0.6554854\n",
      "Train_acc:  1540 0.96209973\n",
      "Test_acc:  1540 0.65588003\n",
      "Train_acc:  1550 0.9625354\n",
      "Test_acc:  1550 0.65745854\n",
      "Train_acc:  1560 0.9625354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc:  1560 0.6578532\n",
      "Train_acc:  1570 0.96297103\n",
      "Test_acc:  1570 0.6582478\n",
      "Train_acc:  1580 0.9631888\n",
      "Test_acc:  1580 0.65745854\n",
      "Train_acc:  1590 0.9636245\n",
      "Test_acc:  1590 0.6578532\n",
      "Train_acc:  1600 0.9636245\n",
      "Test_acc:  1600 0.6582478\n",
      "Train_acc:  1610 0.9644958\n",
      "Test_acc:  1610 0.65982634\n",
      "Train_acc:  1620 0.9647136\n",
      "Test_acc:  1620 0.660221\n",
      "Train_acc:  1630 0.96493137\n",
      "Test_acc:  1630 0.660221\n",
      "Train_acc:  1640 0.965367\n",
      "Test_acc:  1640 0.6606156\n",
      "Train_acc:  1650 0.965367\n",
      "Test_acc:  1650 0.66101027\n",
      "Train_acc:  1660 0.96602046\n",
      "Test_acc:  1660 0.6606156\n",
      "Train_acc:  1670 0.9664561\n",
      "Test_acc:  1670 0.66101027\n",
      "Train_acc:  1680 0.967763\n",
      "Test_acc:  1680 0.66101027\n",
      "Train_acc:  1690 0.96798086\n",
      "Test_acc:  1690 0.66101027\n",
      "Train_acc:  1700 0.9688521\n",
      "Test_acc:  1700 0.66101027\n",
      "Train_acc:  1710 0.96928775\n",
      "Test_acc:  1710 0.66101027\n",
      "Train_acc:  1720 0.96950555\n",
      "Test_acc:  1720 0.6614049\n",
      "Train_acc:  1730 0.96972334\n",
      "Test_acc:  1730 0.6614049\n",
      "Train_acc:  1740 0.96972334\n",
      "Test_acc:  1740 0.6614049\n",
      "Train_acc:  1750 0.9699412\n",
      "Test_acc:  1750 0.66179955\n",
      "Train_acc:  1760 0.97081244\n",
      "Test_acc:  1760 0.66219413\n",
      "Train_acc:  1770 0.97168374\n",
      "Test_acc:  1770 0.66219413\n",
      "Train_acc:  1780 0.97190154\n",
      "Test_acc:  1780 0.66179955\n",
      "Train_acc:  1790 0.97190154\n",
      "Test_acc:  1790 0.6625888\n",
      "Train_acc:  1800 0.97190154\n",
      "Test_acc:  1800 0.6625888\n",
      "Train_acc:  1810 0.97299063\n",
      "Test_acc:  1810 0.6625888\n",
      "Train_acc:  1820 0.9734263\n",
      "Test_acc:  1820 0.6625888\n",
      "Train_acc:  1830 0.97386193\n",
      "Test_acc:  1830 0.6629834\n",
      "Train_acc:  1840 0.9742975\n",
      "Test_acc:  1840 0.66337806\n",
      "Train_acc:  1850 0.9745154\n",
      "Test_acc:  1850 0.66337806\n",
      "Train_acc:  1860 0.97495097\n",
      "Test_acc:  1860 0.6637727\n",
      "Train_acc:  1870 0.9751688\n",
      "Test_acc:  1870 0.6637727\n",
      "Train_acc:  1880 0.9751688\n",
      "Test_acc:  1880 0.6637727\n",
      "Train_acc:  1890 0.97582227\n",
      "Test_acc:  1890 0.66416734\n",
      "Train_acc:  1900 0.97582227\n",
      "Test_acc:  1900 0.664562\n",
      "Train_acc:  1910 0.9762579\n",
      "Test_acc:  1910 0.66495657\n",
      "Train_acc:  1920 0.9766935\n",
      "Test_acc:  1920 0.6653512\n",
      "Train_acc:  1930 0.9766935\n",
      "Test_acc:  1930 0.6653512\n",
      "Train_acc:  1940 0.9766935\n",
      "Test_acc:  1940 0.6653512\n",
      "Train_acc:  1950 0.97712916\n",
      "Test_acc:  1950 0.66574585\n",
      "Train_acc:  1960 0.97712916\n",
      "Test_acc:  1960 0.66653514\n",
      "Train_acc:  1970 0.9775648\n",
      "Test_acc:  1970 0.66653514\n",
      "Train_acc:  1980 0.97800046\n",
      "Test_acc:  1980 0.6669298\n",
      "Train_acc:  1990 0.9786539\n",
      "Test_acc:  1990 0.66811365\n",
      "process time: 39.46848120900613 mins\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from get_data import read_data, to_grayscale, normalise_images\n",
    "\n",
    "#用于计时\n",
    "start = time.clock()\n",
    "\n",
    "#读取数据\n",
    "train_x, train_y = read_data('./data/BelgiumTSC_Training.zip', 'Training/')\n",
    "training_inputs = np.array([np.reshape(x, (3072, 1)) for x in train_x])\n",
    "# print(training_inputs.shape)\n",
    "training_results = np.array([vectorized_result(y) for y in train_y])\n",
    "# print(training_results.shape,len([vectorized_result(y) for y in train_y]))\n",
    "training_data= np.array(list((zip (training_inputs, training_results))))\n",
    "test_x, test_y = read_data('./data/BelgiumTSC_Testing.zip', 'Testing/')\n",
    "test_inputs =np.array([np.reshape(x, (3072, 1)) for x in test_x]) \n",
    "test_results = np.array([vectorized_result(y) for y in test_y])\n",
    "test_data= np.array(list(zip (test_inputs, test_results)))\n",
    "\n",
    "#none可以是任何size；也即，训练集的个数不确定；列数为3072\n",
    "x = tf.placeholder(tf.float32, [None, 3072])\n",
    "y = tf.placeholder(tf.float32, [None, 62])\n",
    "\n",
    "\n",
    "\n",
    "# 权重和偏置；增加一层\n",
    "# 初始值不能为0；否则无法改变权重\n",
    "W1 = tf.Variable(tf.truncated_normal([3072, 800],  stddev=0.5, dtype=tf.float32))\n",
    "B1 = tf.Variable(tf.zeros([800]))\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([800,62], stddev=0.3, dtype=tf.float32))\n",
    "B2 = tf.Variable(tf.zeros([62]))\n",
    "\n",
    "# W3 = tf.Variable(tf.truncated_normal([60,62], stddev=0.1, dtype=tf.float32))\n",
    "# B3 = tf.Variable(tf.zeros([62]))\n",
    "\n",
    "# W4 = tf.Variable(tf.truncated_normal([50,62], stddev=0.5, dtype=tf.float32))\n",
    "# B4 = tf.Variable(tf.zeros([62]))\n",
    "\n",
    "#给定输入，计算输出\n",
    "layer1 = tf.nn.sigmoid(tf.matmul(x, W1) + B1)\n",
    "# layer2 = tf.nn.softmax(tf.matmul(layer1, W2) + B2)\n",
    "# layer3 = tf.nn.softmax(tf.matmul(layer2, W3) + B3)\n",
    "H = tf.nn.softmax(tf.matmul(layer1, W2) + B2)\n",
    "\n",
    "# 损失函数，交叉熵\n",
    "# cross_entropy = tf.reduce_mean(((y * tf.log(H)) +((1 - y) * tf.log(1.0 - H))) * -1)#sigmoid对应的损失函数\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(H), reduction_indices=[1]))#softmax对应的损失函数\n",
    "\n",
    "#梯度下降，步长为0.1\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)\n",
    "\n",
    "# 初始化\n",
    "init = tf.global_variables_initializer()\n",
    "# n=len(training_data)\n",
    "n=training_data.shape[0]\n",
    "n_test=test_data.shape[0]\n",
    "# print(n)\n",
    "# mini_batch_size=1\n",
    "\n",
    "with tf.Session() as s:\n",
    "    s.run(init)\n",
    "    # train\n",
    "    for i in range(2000):\n",
    "        #数据调整\n",
    "        train_xs=np.array([data[0] for data in training_data]).reshape(n,3072)\n",
    "        train_ys=np.array([data[1] for data in training_data]).reshape(n,62)\n",
    "        s.run(train_step, feed_dict={x: train_xs, y: train_ys})\n",
    "\n",
    "        if i%10 == 0:\n",
    "            train_correct_prediction = tf.equal(tf.argmax(H, 1), tf.argmax(y, 1))\n",
    "            train_accuracy = tf.reduce_mean(tf.cast(train_correct_prediction, \"float\"))#tf.cast:类型转换函数\n",
    "#             train_xs=np.array([data[0] for data in test_data]).reshape(n_test,3072)\n",
    "#             test_ys=np.array([data[1] for data in test_data]).reshape(n_test,62)\n",
    "            print(\"Train_acc: \", i,s.run(train_accuracy, feed_dict={x:train_xs, y:train_ys}))            \n",
    "        \n",
    "            correct_prediction = tf.equal(tf.argmax(H, 1), tf.argmax(y, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))#tf.cast:类型转换函数\n",
    "            test_xs=np.array([data[0] for data in test_data]).reshape(n_test,3072)\n",
    "            test_ys=np.array([data[1] for data in test_data]).reshape(n_test,62)\n",
    "            print(\"Test_acc: \", i,s.run(accuracy, feed_dict={x:test_xs, y:test_ys}))\n",
    "\n",
    "#用于计时\n",
    "end = time.clock()\n",
    "print('process time:',str((end - start)/60),'mins')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 测试结果\n",
    "### [3072,400,80,62]\n",
    "* test_acc:15%-16%\n",
    "\n",
    "### [3072,400,80,62]\n",
    "* alpha=0.2,test_acc:2.4%\n",
    "\n",
    "### [3072,400,60,62]\n",
    "* alpha=0.005,test_acc:2.7%\n",
    "* alpha=0.05,test_acc:4.1%\n",
    "* alpha=0.01,test_acc:16.6%\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了21.3mins，train_acc:15.3% ,test_acc:19.3%\n",
    "\n",
    "### [3072,800,62]\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了39.5mins，train_acc:97.8% ,test_acc:66.8%\n",
    "\n",
    "### [3072,600,62]\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了28.6mins，train_acc:96.3% ,test_acc:70.4%\n",
    "\n",
    "### [3072,500,62]\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了30.4mins，train_acc:95.6% ,test_acc:69.9%\n",
    "\n",
    "### [3072,400,62]\n",
    "* 激活函数都是softmax,alpha=0.1,epochs=2000,运行了18.5mins，train_acc:33.5% ,test_acc:34.9%\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=200,运行了2.1mins，train_acc:55.7% ,test_acc:46.8%\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了24.3mins，train_acc:93.6% ,test_acc:69.1%\n",
    "\n",
    "### [3072,200,62]\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=200,运行了2.1mins，train_acc:46.8% ,test_acc:41.1%\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了13mins，train_acc:87.0% ,test_acc:68.0%\n",
    "\n",
    "### [3072,100,62]\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了11.7mins，train_acc:77.2% ,test_acc:62.9%\n",
    "\n",
    "### [3072,120,62]\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了21.2mins，train_acc:80.5% ,test_acc:63.9%\n",
    "\n",
    "### [3072,150,62]\n",
    "* 输出层是softmax,其他激活函数是sigmoid,alpha=0.1,epochs=2000,运行了15.3mins，train_acc:81.1% ,test_acc:67.2%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 同样的神经网络模型，tensorflow和自己编写的代码，准确率差别很大；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
